{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Guide To Principal Component Analysis\n",
    "\n",
    "Who Are You? Gordon Fleetwood\n",
    "\n",
    "Background? Math\n",
    "\n",
    "What Do You Do? Data Science Associate At O.D.S.C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.C.A Intro 1.1\n",
    "\n",
    "P.C.A is an unsupervised linear transformation technique for dimensionality reduction. It eliminates structural redundancies without sacrificing information if there are highly correlated variables. It can be used to:\n",
    "\n",
    "Explore data's structure\n",
    "\n",
    "Visualize high dimensional data\n",
    "\n",
    "Preprocess data for use in other algorithms\n",
    "\n",
    "Compress images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.C.A Intro 1.2\n",
    "\n",
    "P.C.A finds features which are uncorrelated directions of maximum variance in the data, i.e the principal components. These features are linear combinations of the original data and are used to construct the new feature space to project the data into.\n",
    "\n",
    "<img src=\"pictures/pca.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.C.A Intro 1.3\n",
    "This combats:\n",
    "\n",
    "Computational and storage costs\n",
    "\n",
    "Multicollinearity\n",
    "\n",
    "The Curse of Dimensionality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Implementation 1.1\n",
    "\n",
    "Given a n x p data matrix X, there are minimum(n,p) distinct principal components. The first principal component, Z1Z1, is related to the features X1,X2,⋯XpX1,X2,⋯Xp by:\n",
    "\n",
    "    Z1=β11X1+β21X2+⋯+βp1Xp\n",
    "  \n",
    "    Z1=β11X1+β21X2+⋯+βp1Xp\n",
    "\n",
    "    β1=(β11,β21,⋯,βp1)Tβ1=(β11,β21,⋯,βp1)T is called the direction or loadings of Z1Z1. The goal is to find β1β1 to maximize the variance of Z1Z1 under the constraint ∑pj=1β2j1=1∑j=1pβj12=1.\n",
    "\n",
    "    Similarly, the second principal component is represented as:\n",
    "\n",
    "        Z2=β12X1+β22X2+⋯+βp2Xp\n",
    "\n",
    "        Z2=β12X1+β22X2+⋯+βp2Xp\n",
    "\n",
    "        The constraint ∑pj=1β2j2=1∑j=1pβj22=1 is in effect once again. A second constraint, cov(Z1,Z2)=0cov(Z1,Z2)=0 is also added. This ensures that the first and second principal components are orthogonal and thus uncorrelated.\n",
    "\n",
    "        All subsequent principal components are also orthogonal to each other and have the sum of squares of their betas equal to one. The relationship between their variances is the following:\n",
    "\n",
    "            Var(Z1)≥Var(Z2)≥...≥Var(Zp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Implementation 1.2.1\n",
    "\n",
    "The loadings are the eigenvectors of the data's covariance matrix and the amount of variance each component captures is represented by the magnitude of each eigenvector's corresponding eigenvalue. As a reminder, if the following relationship exists for a matrix A and a vector x:\n",
    "\n",
    "    Ax=λx\n",
    "\n",
    "    Ax=λx\n",
    " \n",
    " Then x is an eigenvector of the matrix A and  λλ  is its corresponding eigenvalue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Implementation 1.2.2\n",
    "\n",
    "Thus, PCA can be summarized in 5 steps:\n",
    "\n",
    "    Normalize the data.\n",
    "\n",
    "    Calculate the covariance matrix and find its eigenvalues and eigenvectors.\n",
    "\n",
    "    Select the k eigenvectors that correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace.\n",
    "\n",
    "    Construct a p x k projection matrix from the k eigenvectors.\n",
    "\n",
    "    Use the project matrix to transform the data into the new k-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing k Intelligently\n",
    "\n",
    "The number of principal components used comes down to striking a balance between the total amount of variance that is captured and the number of principal components selected. Three techniques are:\n",
    "\n",
    "    Variance Explained: Choose the number of principal components that match the amount of variance you want explained.\n",
    "\n",
    "        Scree Test: Inspect the eigenvalue x principal component plot and retain the number of principal components corresponding to a drastic drop-off.\n",
    "\n",
    "            Kaiser-Harris Criterion: Retain the number of principal components with eigenvalues over 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Free Lunch\n",
    "\n",
    "Pros:\n",
    "\n",
    "    Reduces the data's complexity and structural redundancy in its features.\n",
    "\n",
    "    Non-parametric so the answer is unique and independent of parameters.\n",
    "\n",
    "    Useful for visualizing high dimensional data.\n",
    "\n",
    "    Models created after using P.C.A are simpler so the variance is lower.\n",
    "\n",
    "    Cons:\n",
    "\n",
    "        Being non-parametric makes results hard to interpret in view of the original data.\n",
    "\n",
    "        Often computationally expensive.\n",
    "\n",
    "        Models created after using P.C.A are simpler so the bias is higher.\n",
    "\n",
    "        Cannot detect non-linear features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.C.A Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here be data\n",
    "\n",
    "import numpy as np\n",
    "x= np.array([[-1,-1,-1,],[-2,-1,-1.5],[-3,-2,-2]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Visualization in Original Dimensional Space\n",
    "\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode,iplot\n",
    "from plotly.graph_objs import*\n",
    "init_notebook_mode()\n",
    "\n",
    "trace1 =Scatter3d(x=x[0],y=x[1], z=x[2], mode='markers')\n",
    "iplot([trace1], link_text='Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    " \n",
    "x_scaled = np.array(list(map(lambda y: (y-np.mean(y))/np.std(y),x.T))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the covariance matrix and calculate its eignvalue and eignvectors\n",
    "\n",
    "cov_matrix = np.cov(x_scaled.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
    "print(cov_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the top k eignvectors of the covariance matrix where k is the size of the desired space.\n",
    "eigen_pairs = [(np.abs(eig_vals[i]),eig_vecs[:,i])for i in range(len(eigs_value))]\n",
    "eigen_pairs.sort(reverse =True)\n",
    "for (eig_val, eig_vec) in eigen_pairs:\n",
    "    print('Eigen value: {0}\\nEigen vector: {1}'.format(eig_val,eig_vec),end='\\n\\n')\n",
    "projection_matrix = np.hstack((eigen_pairs[0][1][:, npneaxis], eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Projection Matrix:\\n\\n', projection_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Project the data into this lower subspace.\n",
    "\n",
    "x_pca_manual = np.dot(x_scaled, projection_matrix)\n",
    "x_pca_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explained Variance\n",
    "\n",
    "eigs_vals.sort()\n",
    "pca_eig_vals =eig_vals[::-1][:-1]/sum(eig_vals)\n",
    "print('Explained Variance By Principal Component:',pca_eig_vals)\n",
    "print('Total Explained Varianece', sum(pca_eig_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#VIisualization in New Subspace\n",
    "scatter= Scatter(x= x_pca_manual[:,0],y=x_pca_manual[:,1],mode='markers')\n",
    "layout = Layout(xaxis=dict(title='1st P.C'), yaxis=dict(title='2nd P.C'))\n",
    "iplot(Figure(data=[scatter], layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.C.A With Scitkit-learn\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(PCA.__doc__)\n",
    "\n",
    "Methods:\n",
    "\n",
    "    set_params: Set the parameters of this estimator.\n",
    "\n",
    "    fit: Fit the model with X.\n",
    "    \n",
    "    transform: Apply the dimensionality reduction on X.\n",
    "\n",
    "    fit_transform: Fit the model with X and apply the        dimensionality reduction on X.\n",
    "   \n",
    "    inverse_transform: Transform data back to its original space.\n",
    "\n",
    "    get_params: Get parameters for this estimator.\n",
    "    \n",
    "    get_covariance: Compute data covariance with the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA In Sklearn\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandScaler\n",
    "\n",
    "x_scaled =StandardScaler().fit_transform(x)\n",
    "pca =PCA(n_components=2)\n",
    "print(x_pca,end='\\n\\n')\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences: Manual vs Sklearn\n",
    "\n",
    "I. Covariance calculation:\n",
    "\n",
    "Sklearn:\n",
    "\n",
    "σij=1n∑ni=1(xi−xi¯)(xj−xj¯)σij=1n∑i=1n(xi−xi¯)(xj−xj¯) \n",
    "\n",
    "Numpy:\n",
    "\n",
    "σij=1n−1∑ni=1(xi−x¯)(xj−xj¯)σij=1n−1∑i=1n(xi−x¯)(xj−xj¯) \n",
    "\n",
    "II. pca.components_ is the transpose of the principal components calculated manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.C.A With The Wine Data Set\n",
    "\n",
    "Goal: Use chemical analysis to the origin of three different wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inspect the Data\n",
    "\n",
    "import pandas as pd\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', \n",
    "                      header = None)\n",
    "cols = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity',  'Magnesium', 'Total phenols', \n",
    "        'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "        'OD280/OD315 of diluted wines', 'Proline']\n",
    "df_wine.columns = cols\n",
    "\n",
    "print('Shape:' ,df_wine.shape, end='\\n\\n')\n",
    "print(df_wine.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline Accuracy\n",
    "\n",
    "df_wine.Class.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Correlation Heat Map\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(df_wine.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#P.C.A Logistic Regression with Two Components\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X,y = df_wine.iloc[:,1:].values, df_wine.iloc[:,0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3),random state=0\n",
    "\n",
    "sc =StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.fit_transform(X_test)\n",
    "\n",
    "pca_2_comps =PCA(n_components=2)\n",
    "X_train_pca_2_comps =pca_2_comps.fit_transform(X_train_std)\n",
    "\n",
    "print('Variance Explained Per PriNCIPAL cComponent:', pca_2_comps.explained_variance_ratio_,end='\\n\\n')\n",
    "print('Total Variance Explained:', np.sum(pca_2_comps.explained_variance_ratio_),end='\\n\\n)\n",
    "\n",
    "clf_logreg = LOgisticRegression()\n",
    "clf_logreg.fit(X_train_pca_2_comps, y_train)\n",
    "pca_2_comps_score = clf_logreg.score(X_train_pca_2_comps, y_train)\n",
    "print('Trianing Accuracy:' , pca_2_comps_score)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mlxtend.evaluate import plot_decision_regions\n",
    "plot_decision_regions(X_train_pca_2_comps, y_train, clf = clf_logreg)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Decision Boundaries')\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(pca_2_comps.components_.T[:,0], pca_2_comps.components_.T[:,1])\n",
    "plt.title('Feature Contribution to Principal Components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "for i, xy in enumerate(pca_2_comps.components_.T):                                  \n",
    "    plt.annotate('(%s)' % cols[i+1], xy=xy, textcoords='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparision Vs Accuracy of Pairs of Features\n",
    "\n",
    "from itertools import combinations\n",
    "accuracy = []\n",
    "for i in combinations([i for i in range(0, 13)], r = 2):\n",
    "    clf_logreg.fit(X_train[:, i], y_train)\n",
    "    if clf_logreg.score(X_train[:, i], y_train) >= pca_2_comps_score:\n",
    "        accuracy.append((i, clf_logreg.score(X_train, y_train)))\n",
    "print('Number of Two Feature Combinations Which Outperform 2 Comp. P.C.A: ', len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Naive Model Accuracy\n",
    "\n",
    "clf_logreg.fit(X_train, y_train)\n",
    "clf_logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart P.C.A With The Wine Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explained Variance Ratio And Scree Test\n",
    "\n",
    "pca_full = PCA(n_components =None)\n",
    "pca_full.fit(X_train_std)\n",
    "\n",
    "plt.bar(range(1,14),\n",
    "        pca_full.explained_variance_ratio_,\n",
    "        alpha=0.5,\n",
    "        align='center',\n",
    "        label = 'indiviual explained variance')\n",
    "plt.step(range(1,14),\n",
    "        np.cumsum(pca_full.explained_variance_ratio_),\n",
    "        where='mid',\n",
    "        label = 'cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kaiser-Harris Criterion\n",
    "\n",
    "def kaiser_harris_criterion(cov_mat):\n",
    "    e_vals, _ = np.linalg.eig(cov_mat)\n",
    "    return len(e_vals[e_vals > 1])\n",
    "\n",
    "print('Kaiser-Harris Criterion: Use {} principal components.'.format(kaiser_harris_criterion(np.cov(X_train_std.T))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Smart P.C.A Logistic Regression vs Naive Logistic Regression\n",
    "\n",
    "pca_4_comps = PCA(n_components = 4)\n",
    "X_train_pca_4_comps = pca_4_comps.fit_transform(X_train_std)\n",
    "\n",
    "clf_logreg.fit(X_train_pca_4_comps, y_train)\n",
    "print('P.C.A Logistic Regression Training Accuracy:',clf_logreg.score(X_train_pca_4_comps, y_train))\n",
    "print('P.C.A Logistic Regression Testing Accuracy:',clf_logreg.score(pca_4_comps.transform(X_test_std), y_test))\n",
    "print('\\n')\n",
    "clf_logreg.fit(X_train, y_train)\n",
    "print('Naive Logistic Regression Testing Accuracy:',clf_logreg.score(X_train, y_train))\n",
    "print('Naive Logistic Regression Testing Accuracy:',clf_logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [KDD Cup 1999](http://www.kdd.org/kdd-cup/view/kdd-cup-1999/Tasks): Computer Network Intrusion Detection\n",
    "\n",
    ">The task for the classifier learning contest organized in conjunction with the KDD'99 conference was to learn a predictive model (i.e. a classifier) capable of distinguishing between legitimate and illegitimate connections in a computer network.\n",
    "\n",
    "* Attacks fall into four main categories: denial-of-service,  unauthorized access from a remote machine, unauthorized access to local superuser (root) privileges, and probing.\n",
    "\n",
    "\n",
    "* The datasets contain a total of 24 training attack types, with an additional 14 types in the test data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get The Data\n",
    "\n",
    "#Note: This code is different from the talk. There I was reading in the data locally. Here I'm doing it from the url.\n",
    "\n",
    "import requests, zipfile, io\n",
    "res_kdd = requests.get('http://kdd.org/cupfiles/KDDCupData/1999/kddcup.data_10_percent.zip')\n",
    "file_kdd = zipfile.ZipFile(io.BytesIO(res_kdd.content))\n",
    "file_kdd_access = file_kdd.open(\"kddcup.data_10_percent.txt\")\n",
    "\n",
    "cols = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\", \"dst_bytes\",\"land\",\n",
    "        \"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\", \"logged_in\",\"num_compromised\",\n",
    "        \"root_shell\",\"su_attempted\",\"num_root\", \"num_file_creations\",\"num_shells\",\n",
    "        \"num_access_files\",\"num_outbound_cmds\", \"is_host_login\",\"is_guest_login\",\"count\",\n",
    "        \"srv_count\",\"serror_rate\", \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\n",
    "        \"same_srv_rate\", \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\n",
    "        \"dst_host_srv_count\", \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\n",
    "        \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
    "        \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"TARGET\"]\n",
    "\n",
    "kdd_data = pd.read_csv(file_kdd_access, header=None, names=cols, low_memory=False)\n",
    "kdd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('10% of the data is {} data points and has {} columns.'.format(len(kdd_data), len(kdd_data.columns)), end = '\\n\\n')\n",
    "print('Missingness: ', end = '\\n\\n')\n",
    "print(kdd_data.isnull().sum())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('There are {} unique targets'.format(len(kdd_data.TARGET.unique())), end = '\\n\\n')\n",
    "kdd_data.TARGET.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adjusted Baseline Accuracy\n",
    "\n",
    "kdd_data['BINARY_TARGET'] = kdd_data['TARGET'].map(lambda x: x if x=='normal.' else 'abnormal.')\n",
    "kdd_data.BINARY_TARGET.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Column Types\n",
    "\n",
    "kdd_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Discrepancies\n",
    "\n",
    "weird_cols = ['num_root', 'num_file_creations', 'num_shells']\n",
    "for col in weird_cols:\n",
    "    print(col)\n",
    "    print(list(filter(lambda x: x[1]%2==0, [(x,x.isdigit()) for x in kdd_data[col].values])))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fix Discrepancies: Replace By Most Frequent Value\n",
    "\n",
    "#Side Note: As people mention in the talk, dropping these rows is an option to consider as the whole row could be contaminated\n",
    "#if this is a data reading error.\n",
    "\n",
    "kdd_data.loc[kdd_data.num_root == 'tcp', 'num_root'] = kdd_data.num_root.value_counts().index[0]\n",
    "kdd_data.loc[kdd_data.num_file_creations == 'http', 'num_file_creations'] = kdd_data.num_file_creations.value_counts().index[0]\n",
    "kdd_data.loc[kdd_data.num_shells == 'SF', 'num_shells'] = kdd_data.num_shells.value_counts().index[0]\n",
    "\n",
    "kdd_data.loc[:, weird_cols] = kdd_data.loc[:, weird_cols].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#su_attempted: Unique values are 0,1,2 but should be 0,1\n",
    "\n",
    "kdd_data.loc[kdd_data.su_attempted == 2, 'su_attempted'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Numerical Feature Summary\n",
    "\n",
    "kdd_data_num_features = kdd_data.iloc[:,:-2].select_dtypes(exclude = ['object'])\n",
    "kdd_data_cat_features = kdd_data.iloc[:,:-2].select_dtypes(include = ['object'])\n",
    "\n",
    "kdd_data_num_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Categorical Feature Summary\n",
    "\n",
    "kdd_data_cat_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation Heat Map\n",
    "\n",
    "sns.heatmap(kdd_data_num_features.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding Categorical Features\n",
    "\n",
    "def column_encoding(df_num, df_cat):\n",
    "    for i in range(len(df_cat.columns)):\n",
    "        df_num = pd.concat([df_num, pd.get_dummies(df_cat.iloc[:,i])], axis=1)\n",
    "    return df_num\n",
    "\n",
    "kdd_features_dummied = column_encoding(kdd_data_num_features, kdd_data_cat_features)\n",
    "len(kdd_features_dummied.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory P.C.A¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "scaled_X_dummied = StandardScaler().fit_transform(kdd_features_dummied)\n",
    "pca_kdd = PCA(n_components = 2)\n",
    "scaled_X_dummied_pca_2 = pca_kdd.fit_transform(scaled_X_dummied)\n",
    "\n",
    "normal_indices = kdd_data[kdd_data.TARGET!='normal.'].index.values\n",
    "abnormal_indices = kdd_data[kdd_data.TARGET=='normal.'].index.values\n",
    "plt.scatter(scaled_X_dummied_pca_2[normal_indices,0], scaled_X_dummied_pca_2[normal_indices,1], c= 'b', )\n",
    "plt.scatter(scaled_X_dummied_pca_2[abnormal_indices,0], scaled_X_dummied_pca_2[abnormal_indices,1], c= 'r')\n",
    "plt.title('Network Intrusion In 2D')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(['normal', 'abnormal'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose Subspace Dimension\n",
    "k_harris_rec = kaiser_harris_criterion(np.cov(scaled_X_dummied.T))\n",
    "print('Kaiser-Harris Criterion: Use {} principal components.'.format(k_harris_rec))\n",
    "\n",
    "pca_kdd.set_params(n_components = k_harris_rec)\n",
    "scaled_X_dummied_pca_k_harris = pca_kdd.fit_transform(scaled_X_dummied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing A Classifier\n",
    "\n",
    "import sklearn.cross_validation as cv\n",
    "y_train = kdd_data.iloc[:,-1].map(lambda x: 1 if x=='normal.' else 0)\n",
    "stratified_divide = cv.StratifiedKFold(y_train, n_folds=10, random_state=1)\n",
    "clf_log_reg2 = LogisticRegression()\n",
    "clf_log_reg2_cv_score = np.mean(cv.cross_val_score(clf_log_reg2, \n",
    "                                                   scaled_X_dummied_pca_k_harris, \n",
    "                                                   y_train, \n",
    "                                                   cv = stratified_divide, \n",
    "                                                   scoring = 'accuracy'))\n",
    "print(clf_log_reg2_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "import sklearn.metrics as met\n",
    "met.confusion_matrix(y_train, \n",
    "                     (clf_log_reg2.fit(scaled_X_dummied_pca_k_harris, y_train)\n",
    "                                  .predict(scaled_X_dummied_pca_k_harris)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Be Continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Notes\n",
    "\n",
    "An Intro To Statistical Learning\n",
    "\n",
    "P.C.A Explained Visually\n",
    "\n",
    "P.C.A Regression\n",
    "\n",
    "Sklearn TSNE\n",
    "\n",
    "Email: gordonfleetwood@yahoo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
